{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d2716cf",
   "metadata": {},
   "source": [
    "## 머신러닝 기초"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "8c8fc7ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "y = x ** 2\n",
    "y.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "c0f33ce2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7,)"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "tsor = np.arange(7.0)\n",
    "tsor.ndim # 1.    # 텐서의 차원\n",
    "tsor.shape # (7,)  # 텐서의 크기\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "0c3576d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 3)"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tsor2 = np.arange(1.0, 13).reshape((4,3))\n",
    "tsor2.ndim #2\n",
    "tsor2.shape # (4,3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "2e1dde4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "tsor3 = torch.arange(0.0, 7)\n",
    "tsor3.dim() # 1\n",
    "tsor3.shape  # torch.Size([7])\n",
    "tsor3.size() # torch.Size([7])\n",
    "tsor3.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "9443fb12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.,  2.],\n",
       "        [ 4.,  5.],\n",
       "        [ 7.,  8.],\n",
       "        [10., 11.]])"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.arange(1,13.0).reshape((4,3))\n",
    "t.dim() #2\n",
    "t.size() # torch.Size([4, 3])\n",
    "t.shape # torch.Size([4, 3])\n",
    "\n",
    "t[:,1]\n",
    "t[:,1].size()\n",
    "t[:,:-1] # 마지막 차원 제외\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "4ba1c5a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5., 5.])"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m1 = torch.FloatTensor([3,3])\n",
    "m2 = torch.FloatTensor([2,2])\n",
    "m1 + m2 # tensor([5., 5.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "ffcbc1b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4., 5.])"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m1 = torch.FloatTensor([1,2])\n",
    "m2 = torch.FloatTensor([3])\n",
    "m1 + m2 # tensor([4., 5.]) 큰 size에 맞춰서 작은 사이즈가 중복되어 생김"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "342198b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2., 3.],\n",
       "        [5., 6.]])"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m1 = torch.FloatTensor([[1,2],[3,4]]) # 2*2\n",
    "m2 = torch.FloatTensor([[1],[2]]) # 2*1\n",
    "m1 + m2 # tensor([[2., 3.],\n",
    "        #         [5., 6.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "b1aacb5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 5.],\n",
       "        [11.]])"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m1 = torch.FloatTensor([[1,2],[3,4]]) # 2*2\n",
    "m2 = torch.FloatTensor([[1],[2]]) # 2*1\n",
    "\n",
    "m1.matmul(m2).shape # torch.Size([2, 1]) 내적 (dot 연산) -> matrix multiplication \n",
    "(m1 @ m2).shape # 위와 같은 연산\n",
    "torch.matmul(m1,m2)\n",
    "# torch.dot(m1,m2) 벡터끼리의 내적만 지원함 행렬곱은 안됨 -> 1D 전용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d77b36",
   "metadata": {},
   "source": [
    "#### 평균"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "798c2b67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.5000, 3.5000])"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.FloatTensor([1,2])\n",
    "t.mean() # tensor(1.5000)\n",
    "m1 = torch.FloatTensor([[1,2],[3,4]]) # 2*2\n",
    "m1.mean()\n",
    "\n",
    "m1.mean(dim=0 ,dtype=torch.float) # tensor([2., 3.])\n",
    "m1.mean(dim=1 ,dtype=torch.float) # tensor([1.5000, 3.5000])\n",
    "# m1.mean(dim=2 ,dtype=torch.float) # Dimension out of range (expected to be in range of [-2, 1], but got 2)\n",
    "\n",
    "# dim=0이면 0번째 축(행 방향) 을 없애면서 계산,\n",
    "# dim=1이면 1번째 축(열 방향) 을 없애면서 계산\n",
    "m1.mean(dim=-1 ,dtype=torch.float) # tensor([1.5000, 3.5000])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a46c61",
   "metadata": {},
   "source": [
    "#### 덧셈\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "2ba69c1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3., 7.])"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2 = torch.FloatTensor([[1,2],[3,4]])\n",
    "t2.sum() #tensor(10.)\n",
    "t2.sum(dim=0) # tensor([4., 6.])\n",
    "t2.sum(dim=1) # tensor([3., 7.])\n",
    "t2.sum(dim=-1) # tensor([3., 7.])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96421c8e",
   "metadata": {},
   "source": [
    "#### max, arxmax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "350e8495",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1])"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t3 = torch.FloatTensor([[1,2],[3,4]])\n",
    "t3.max() # tensor(4.)\n",
    "t3.max(dim=0) # torch.return_types.max(values=tensor([3., 4.]),indices=tensor([1, 1])) # indice= 최대값위치\n",
    "t3.max(dim=1) # torch.return_types.max(values=tensor([2., 4.]),(indices=tensor([1, 1]))\n",
    "\n",
    "t3.max(dim=0)[0] # tensor([3., 4.]) -> tensor\n",
    "t3.max(dim=0)[1] # tensor([1, 1]) -> indice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a70194",
   "metadata": {},
   "source": [
    "#### view - 원소의 수를 유지하며 텐서 크기 변경 (np.reshape())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "f764e47d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  2.],\n",
       "        [ 3.,  4.,  5.],\n",
       "        [ 6.,  7.,  8.],\n",
       "        [ 9., 10., 11.]])"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "tt = np.arange(0,12).reshape((4,3))\n",
    "tt\n",
    "\n",
    "ft = torch.FloatTensor(tt)\n",
    "\n",
    "# tensor([[ 0.,  1.,  2.],\n",
    "#         [ 3.,  4.,  5.],\n",
    "#         [ 6.,  7.,  8.],\n",
    "#         [ 9., 10., 11.]])\n",
    "\n",
    "\n",
    "ft.shape # torch.Size([4, 3])\n",
    "ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "ba3e319a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.],\n",
       "        [ 2.,  3.],\n",
       "        [ 4.,  5.],\n",
       "        [ 6.,  7.],\n",
       "        [ 8.,  9.],\n",
       "        [10., 11.]])"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft.view([-1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "47f1825c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 1, 3])"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft.view([-1,1,3]).shape # torch.Size([4, 1, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c044b6ca",
   "metadata": {},
   "source": [
    "#### squeeze -1 차원 제거, unsqueeze +1 차원추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "dff4ac5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1, 1])"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ttt = ttt = torch.arange(1,4).reshape((3,1))\n",
    "\n",
    "ttt.shape #torch.Size([3, 1])\n",
    "ttt.squeeze().shape # torch.Size([3])\n",
    "\n",
    "\n",
    "tttt = torch.arange(1,4).reshape((3,1))\n",
    "# tensor([[1],\n",
    "#         [2],\n",
    "#         [3]])\n",
    "# torch.Size([3, 1])\n",
    "\n",
    "tttt.unsqueeze(0) # 인덱스가 0부터 시작하므로 0은 첫번째 차원을 의미한다.\n",
    "\n",
    "# tensor([[[1],\n",
    "#          [2],\n",
    "#          [3]]])\n",
    "# torch.Size([1, 3, 1])\n",
    "\n",
    "tttt.unsqueeze(1)\n",
    "\n",
    "# tensor([[[1]],\n",
    "\n",
    "#         [[2]],\n",
    "\n",
    "        # [[3]]])\n",
    "tttt.unsqueeze(1).shape\n",
    "\n",
    "#torch.Size([3, 1, 1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "9c34597b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3]])"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tttt.unsqueeze(-1).shape # torch.Size([3, 1, 1])\n",
    "\n",
    "tttt.view(1,-1) # torch.Size([1, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460bdb0a",
   "metadata": {},
   "source": [
    "#### 타입캐스팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "2d809128",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 0., 0., 1.])"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft = torch.LongTensor([1,2,3,4]) #tensor([1, 2, 3, 4])\n",
    "ft.float() # tensor([1., 2., 3., 4.])\n",
    "\n",
    "bt = torch.ByteTensor([True, False, False, True]) # tensor([1, 0, 0, 1], dtype=torch.uint8)\n",
    "bt.long() # tensor([1, 0, 0, 1])\n",
    "bt.float() # tensor([1., 0., 0., 1.])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe31633",
   "metadata": {},
   "source": [
    "#### tensor 연산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "ac7407cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2.],\n",
       "        [3., 4.],\n",
       "        [5., 6.],\n",
       "        [7., 8.]])"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.FloatTensor([[1,2],[3,4]])\n",
    "y = torch.FloatTensor([[5,6],[7,8]])\n",
    "\n",
    "torch.cat((x,y)) # dim= 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "cf9f4cf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2., 5., 6.],\n",
       "        [3., 4., 7., 8.]])"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat((x,y),dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39978e8a",
   "metadata": {},
   "source": [
    "#### 스태킹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "38524749",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 4.],\n",
       "        [2., 5.],\n",
       "        [3., 6.]])"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.FloatTensor([1,4])\n",
    "y = torch.FloatTensor([2,5])\n",
    "z = torch.FloatTensor([3,6])\n",
    "\n",
    "torch.stack((x,y,z)) #1+1+1 차원 -> 2차원\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "bcd5441c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 4.],\n",
       "        [2., 5.],\n",
       "        [3., 6.]])"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat((x.unsqueeze(0),y.unsqueeze(0), z.unsqueeze(0))) # 위와 같다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1dab2b",
   "metadata": {},
   "source": [
    "#### one_like, zeros_like -0, 1로 채워진 텐서"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "396b34a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o = torch.FloatTensor([[0,1,2],[2,1,0]]) # 2 * 3 텐서\n",
    "torch.ones_like(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "54be3796",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.],\n",
       "        [0., 0., 0.]])"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros_like(o)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f03d90",
   "metadata": {},
   "source": [
    "#### in place operation 연산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "e0670f17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2., 4.],\n",
       "        [6., 8.]])"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.FloatTensor([[1,2],[3,4]])\n",
    "x.mul(2)\n",
    "x # 원본은 그대로임\n",
    "x.mul_(2)\n",
    "x # 원본이 변경됨( in_place version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00198eef",
   "metadata": {},
   "source": [
    "## 선형회귀와 다중미분"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "0ea5d911",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "x_train = torch.FloatTensor([[1],[2],[3]]) # 3*1\n",
    "y_train = torch.FloatTensor([[2],[4],[6]]) # 3*1\n",
    "\n",
    "x_train.shape # torch.Size([3, 1])\n",
    "\n",
    "# 가중치 W를 0으로 초기화하고 학습을 통해 값이 변경되는 변수임을 명시\n",
    "\n",
    "W = torch.zeros(1, requires_grad=True) # tensor([0.], requires_grad=True)\n",
    "W3 = torch.zeros(0, requires_grad=True) # tensor([], requires_grad=True)\n",
    "W2 = torch.zeros((3,3), requires_grad=True) \n",
    "\n",
    "# tensor([[0., 0., 0.],\n",
    "#         [0., 0., 0.],\n",
    "#         [0., 0., 0.]], requires_grad=True)\n",
    "\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "\n",
    "# 경사하강법 적용할 optimizer 함수 SGD를 정의\n",
    "\n",
    "optimizer = optim.SGD([W,b],lr=0.01) # SGD - Stochastic Gradient Descent\n",
    "\n",
    "hypothesis = x_train * W + b\n",
    "\n",
    "# tensor([[0.],\n",
    "#         [0.],\n",
    "#         [0.]], grad_fn=<AddBackward0>)\n",
    "\n",
    "# 비용함수 = average(가설값 - 실제데이터)^2\n",
    "\n",
    "cost = torch.mean((hypothesis - y_train) ** 2)\n",
    "# tensor(18.6667, grad_fn=<MeanBackward0>)\n",
    "\n",
    "# gradient 0으로 초기화\n",
    "\n",
    "optimizer.zero_grad()\n",
    "\n",
    "# 비용함수를 미분하여 gradient 계산\n",
    "\n",
    "cost.backward()\n",
    "\n",
    "# W, b 를 업데이트\n",
    "\n",
    "optimizer.step()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15525f4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/2000 W: 0.002091, b: 0.001, Cost: 18.666666\n",
      "Epoch  100/2000 W: 0.198613, b: 0.085, Cost: 14.574195\n",
      "Epoch  200/2000 W: 0.372262, b: 0.159, Cost: 11.382535\n",
      "Epoch  300/2000 W: 0.525710, b: 0.224, Cost: 8.893384\n",
      "Epoch  400/2000 W: 0.661319, b: 0.281, Cost: 6.952098\n",
      "Epoch  500/2000 W: 0.781174, b: 0.332, Cost: 5.438072\n",
      "Epoch  600/2000 W: 0.887117, b: 0.376, Cost: 4.257249\n",
      "Epoch  700/2000 W: 0.980772, b: 0.415, Cost: 3.336283\n",
      "Epoch  800/2000 W: 1.063577, b: 0.449, Cost: 2.617969\n",
      "Epoch  900/2000 W: 1.136801, b: 0.479, Cost: 2.057692\n",
      "Epoch 1000/2000 W: 1.201561, b: 0.505, Cost: 1.620668\n",
      "Epoch 1100/2000 W: 1.258849, b: 0.528, Cost: 1.279763\n",
      "Epoch 1200/2000 W: 1.309536, b: 0.548, Cost: 1.013818\n",
      "Epoch 1300/2000 W: 1.354394, b: 0.566, Cost: 0.806334\n",
      "Epoch 1400/2000 W: 1.394105, b: 0.581, Cost: 0.644440\n",
      "Epoch 1500/2000 W: 1.429270, b: 0.594, Cost: 0.518100\n",
      "Epoch 1600/2000 W: 1.460421, b: 0.605, Cost: 0.419488\n",
      "Epoch 1700/2000 W: 1.488025, b: 0.615, Cost: 0.342501\n",
      "Epoch 1800/2000 W: 1.512498, b: 0.624, Cost: 0.282381\n",
      "Epoch 1900/2000 W: 1.534204, b: 0.631, Cost: 0.235414\n",
      "Epoch 2000/2000 W: 1.553468, b: 0.637, Cost: 0.198704\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "x_train = torch.FloatTensor([[1],[2],[3]]) # 3*1\n",
    "y_train = torch.FloatTensor([[2],[4],[6]]) # 3*1\n",
    "\n",
    "x_train.shape # torch.Size([3, 1])\n",
    "\n",
    "# 가중치 W를 0으로 초기화하고 학습을 통해 값이 변경되는 변수임을 명시\n",
    "\n",
    "W = torch.zeros(1, requires_grad=True) # tensor([0.], requires_grad=True)\n",
    "\n",
    "\n",
    "# tensor([[0., 0., 0.],\n",
    "#         [0., 0., 0.],\n",
    "#         [0., 0., 0.]], requires_grad=True)\n",
    "\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "\n",
    "# 경사하강법 적용할 optimizer 함수 SGD를 정의\n",
    "\n",
    "optimizer = optim.SGD([W,b],lr=0.00112) # SGD - Stochastic Gradient Descent\n",
    "\n",
    "nb_epochs = 2000 # 원하는 만큼 경사하강법을 반복\n",
    "\n",
    "for epoch in range(nb_epochs +1) :\n",
    "    \n",
    "    hypothesis = x_train * W + b\n",
    "    cost = torch.mean((hypothesis - y_train) ** 2)\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "#   100번 마다 로그 출력\n",
    "    if epoch % 100 == 0 :\n",
    "      print(\"Epoch {:4d}/{} W: {:3f}, b: {:.3f}, Cost: {:6f}\".format(epoch, nb_epochs, W.item(), b.item(), cost.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c871a51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 w1: 0.294 w2: 0.294 w3: 0.297 b: 0.003 Cost:29661.800781\n",
      "Epoch  100/1000 w1: 0.674 w2: 0.661 w3: 0.676 b: 0.008 Cost:1.563628\n",
      "Epoch  200/1000 w1: 0.679 w2: 0.655 w3: 0.677 b: 0.008 Cost:1.497595\n",
      "Epoch  300/1000 w1: 0.684 w2: 0.649 w3: 0.677 b: 0.008 Cost:1.435044\n",
      "Epoch  400/1000 w1: 0.689 w2: 0.643 w3: 0.678 b: 0.008 Cost:1.375726\n",
      "Epoch  500/1000 w1: 0.694 w2: 0.638 w3: 0.678 b: 0.009 Cost:1.319507\n",
      "Epoch  600/1000 w1: 0.699 w2: 0.633 w3: 0.679 b: 0.009 Cost:1.266222\n",
      "Epoch  700/1000 w1: 0.704 w2: 0.627 w3: 0.679 b: 0.009 Cost:1.215703\n",
      "Epoch  800/1000 w1: 0.709 w2: 0.622 w3: 0.679 b: 0.009 Cost:1.167810\n",
      "Epoch  900/1000 w1: 0.713 w2: 0.617 w3: 0.680 b: 0.009 Cost:1.122429\n",
      "Epoch 1000/1000 w1: 0.718 w2: 0.613 w3: 0.680 b: 0.009 Cost:1.079390\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "torch.manual_seed(1)\n",
    "# 훈련 데이터\n",
    "x1_train = torch.FloatTensor([[73], [93], [89], [96], [73]])\n",
    "x2_train = torch.FloatTensor([[80], [88], [91], [98], [66]])\n",
    "x3_train = torch.FloatTensor([[75], [93], [90], [100], [70]])\n",
    "y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])\n",
    "# 가중치 w와 편향 b 초기화\n",
    "w1 = torch.zeros(1, requires_grad=True)\n",
    "w2 = torch.zeros(1, requires_grad=True)\n",
    "w3 = torch.zeros(1, requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "# optimizer 설정\n",
    "optimizer = optim.SGD( [ w1, w2, w3, b] , lr=0.00001 )\n",
    "\n",
    "\n",
    "nb_epochs = 1000\n",
    "for epoch in range(nb_epochs + 1):\n",
    "# H(x) 계산\n",
    "  hypothesis = x1_train * w1 + x2_train * w2 + x3_train * w3 + b\n",
    "# cost 계산\n",
    "  cost = torch.mean((hypothesis - y_train) ** 2)\n",
    "# cost로 H(x) 개선\n",
    "  optimizer.zero_grad()\n",
    "  cost.backward()\n",
    "  optimizer.step()\n",
    "# 100번마다 로그 출력\n",
    "  if epoch % 100 == 0:\n",
    "    print('Epoch {:4d}/{} w1: {:.3f} w2: {:.3f} w3: {:.3f} b: {:.3f} Cost:{:.6f}'.format(\n",
    "\n",
    "  epoch, nb_epochs, w1.item(), w2.item(), w3.item(), b.item(),cost.item()\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d47d978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   20/20 hypothesis: tensor([154.0536, 185.1134, 175.7451, 198.6146, 141.2158]) Cost: 5.957105\n",
      "Predicted value for input [75.0, 85.0, 72.0]: 156.8051300048828\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "torch.manual_seed(1)\n",
    "x_train = torch.FloatTensor([[73, 80, 75],\n",
    "[93, 88, 93],\n",
    "[89, 91, 80],\n",
    "[96, 98, 100],\n",
    "[73, 66, 70]])\n",
    "\n",
    "y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])\n",
    "# 모델 초기화\n",
    "W = torch.zeros((3, 1), requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "# optimizer 설정\n",
    "optimizer = optim.SGD([W, b], lr=1e-5)\n",
    "nb_epochs = 20\n",
    "for epoch in range(nb_epochs + 1):\n",
    "# H(x) 계산, 편향 b는 브로드 캐스팅되어 각 샘플에 더해집니다.\n",
    "  hypothesis = x_train.matmul(W) + b\n",
    "  \n",
    "  # cost 계산\n",
    "  cost = torch.mean((hypothesis - y_train) ** 2)\n",
    "  # cost로 H(x) 개선\n",
    "  optimizer.zero_grad()\n",
    "  cost.backward()\n",
    "  optimizer.step()\n",
    "\n",
    "\n",
    "print('Epoch {:4d}/{} hypothesis: {} Cost: {:.6f}'.format(epoch, nb_epochs, hypothesis.squeeze().detach(), cost.item()\n",
    "))\n",
    "# 학습하여 산출된 가중치( W1~3 과 b값) 에 대하여 임의의 입력 값 적용\n",
    "# no_grad() block 내 모든 연산수행시 역전파(기울기 계산) 비활성화\n",
    "#\n",
    "with torch.no_grad():\n",
    "# 예측하고 싶은 임의의 입력값\n",
    "  new_input = torch.FloatTensor([[75, 85, 72]])\n",
    "  prediction = new_input.matmul(W) + b\n",
    "  \n",
    "print('Predicted value for input {}: {}'.format(new_input.squeeze().tolist(),prediction.item()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa1496e",
   "metadata": {},
   "source": [
    "\n",
    "PyTorch는 기본적으로 모든 Tensor 연산을 추적(trace) 합니다.\n",
    "왜냐하면 backward()로 역전파(gradient 계산)를 하기 위해서죠.\n",
    "\n",
    "하지만 예측할 때는 더 이상 학습하지 않으므로,\n",
    "기울기를 계산할 필요가 없습니다 → 메모리 절약 + 속도 향상.\n",
    "\n",
    "with torch.no_grad():\n",
    "    prediction = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42268ed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter...\n",
      "hello obama\n",
      "hello trump\n",
      "exit...\n"
     ]
    }
   ],
   "source": [
    "class Hello:\n",
    "    def __enter__(self):\n",
    "        # 사용할 자원을 가져오거나 만든다(핸들러 등)\n",
    "        print('enter...')\n",
    "        return self # 반환값이 있어야 VARIABLE를 블록내에서 사용할 수 있다\n",
    "        \n",
    "    def sayHello(self, name):\n",
    "        # 자원을 사용한다. ex) 인사한다\n",
    "        print('hello ' + name)\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        # 마지막 처리를 한다(자원반납 등)\n",
    "        print('exit...')\n",
    "\n",
    "with Hello() as h:\n",
    "    h.sayHello('obama')\n",
    "    h.sayHello('trump')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f737de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[0.5153]], requires_grad=True), Parameter containing:\n",
      "tensor([-0.4414], requires_grad=True)]\n",
      "Epoch    0/1800 Cost: 13.103541\n",
      "Epoch  100/1800 Cost: 0.002791\n",
      "Epoch  200/1800 Cost: 0.001724\n",
      "Epoch  300/1800 Cost: 0.001066\n",
      "Epoch  400/1800 Cost: 0.000658\n",
      "Epoch  500/1800 Cost: 0.000407\n",
      "Epoch  600/1800 Cost: 0.000251\n",
      "Epoch  700/1800 Cost: 0.000155\n",
      "Epoch  800/1800 Cost: 0.000096\n",
      "Epoch  900/1800 Cost: 0.000059\n",
      "Epoch 1000/1800 Cost: 0.000037\n",
      "Epoch 1100/1800 Cost: 0.000023\n",
      "Epoch 1200/1800 Cost: 0.000014\n",
      "Epoch 1300/1800 Cost: 0.000009\n",
      "Epoch 1400/1800 Cost: 0.000005\n",
      "Epoch 1500/1800 Cost: 0.000003\n",
      "Epoch 1600/1800 Cost: 0.000002\n",
      "Epoch 1700/1800 Cost: 0.000001\n",
      "Epoch 1800/1800 Cost: 0.000001\n",
      "훈련 후 입력이 4일 때의 예측값 : tensor([[7.9982]], grad_fn=<AddmmBackward0>)\n",
      "[Parameter containing:\n",
      "tensor([[1.9990]], requires_grad=True), Parameter containing:\n",
      "tensor([0.0023], requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "torch.manual_seed(1) # random seed 고정값\n",
    "# 데이터\n",
    "x_train = torch.FloatTensor([[1], [2], [3]])\n",
    "y_train = torch.FloatTensor([[2], [4], [6]])\n",
    "# 모델을 선언 및 초기화. 단순 선형 회귀이므로 input_dim=1, output_dim=1.\n",
    "model = nn.Linear(1,1)\n",
    "print(list(model.parameters()))\n",
    "# optimizer 설정. 경사 하강법 SGD를 사용하고 learning rate를 의미하는 lr은0.01\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "# 전체 훈련 데이터에 대해 경사 하강법을 2,000회 반복\n",
    "nb_epochs = 1800\n",
    "for epoch in range(nb_epochs+1):\n",
    "# H(x) 계산\n",
    "  prediction = model(x_train)\n",
    "# cost 계산\n",
    "  cost = F.mse_loss(prediction, y_train) # 평균 제곱 오차 함수(pytorch 내장)\n",
    "# cost로 H(x) 개선하는 부분\n",
    "  optimizer.zero_grad() # gradient를 0으로 초기화\n",
    "# 비용 함수를 미분하여 gradient 계산\n",
    "  cost.backward() # backward 연산\n",
    "# W와 b를 업데이트\n",
    "  optimizer.step()\n",
    "  if epoch % 100 == 0:\n",
    "  # 100번마다 로그 출력\n",
    "    print('Epoch {:4d}/{} Cost: {:.6f}'.format(epoch, nb_epochs, cost.item()\n",
    "  ))\n",
    "    \n",
    "# 임의의 입력 4를 선언\n",
    "new_var = torch.FloatTensor([[4.0]])\n",
    "# 입력한 값 4에 대해서 예측값 y를 리턴받아서 pred_y에 저장\n",
    "pred_y = model(new_var) # forward 연산\n",
    "# y = 2x 이므로 입력이 4라면 y가 8에 가까운 값이 나와야 제대로 학습이 된것\n",
    "print(\"훈련 후 입력이 4일 때의 예측값 :\", pred_y)\n",
    "print(list(model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a7433f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[ 0.2975, -0.2548, -0.1119]], requires_grad=True), Parameter containing:\n",
      "tensor([0.2710], requires_grad=True)]\n",
      "Epoch    0/2000 Cost: 31667.597656\n",
      "Epoch  100/2000 Cost: 0.225990\n",
      "Epoch  200/2000 Cost: 0.223910\n",
      "Epoch  300/2000 Cost: 0.221936\n",
      "Epoch  400/2000 Cost: 0.220061\n",
      "Epoch  500/2000 Cost: 0.218269\n",
      "Epoch  600/2000 Cost: 0.216570\n",
      "Epoch  700/2000 Cost: 0.214958\n",
      "Epoch  800/2000 Cost: 0.213411\n",
      "Epoch  900/2000 Cost: 0.211951\n",
      "Epoch 1000/2000 Cost: 0.210564\n",
      "Epoch 1100/2000 Cost: 0.209231\n",
      "Epoch 1200/2000 Cost: 0.207970\n",
      "Epoch 1300/2000 Cost: 0.206765\n",
      "Epoch 1400/2000 Cost: 0.205617\n",
      "Epoch 1500/2000 Cost: 0.204521\n",
      "Epoch 1600/2000 Cost: 0.203483\n",
      "Epoch 1700/2000 Cost: 0.202489\n",
      "Epoch 1800/2000 Cost: 0.201539\n",
      "Epoch 1900/2000 Cost: 0.200637\n",
      "Epoch 2000/2000 Cost: 0.199773\n",
      "훈련 후 입력이 73, 80, 75일 때의 예측값 : tensor([[151.2305]], grad_fn=<AddmmBackward0>)\n",
      "[Parameter containing:\n",
      "tensor([[0.9778, 0.4539, 0.5768]], requires_grad=True), Parameter containing:\n",
      "tensor([0.2802], requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "torch.manual_seed(1)\n",
    "x_train = torch.FloatTensor([[73, 80, 75],\n",
    "  [93, 88, 93],\n",
    "  [89, 91, 90],\n",
    "  [96, 98, 100],\n",
    "  [73, 66, 70]])\n",
    "\n",
    "y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])\n",
    "# 모델을 선언 및 초기화. 다중 선형 회귀이므로 input_dim=3, output_dim=1.\n",
    "model = nn.Linear(3,1)\n",
    "print(list(model.parameters()))\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.00001)\n",
    "nb_epochs = 2000\n",
    "for epoch in range(nb_epochs+1):\n",
    "# H(x) 계산\n",
    "  prediction = model(x_train) #model(x_train)은 model.forward(x_train)와 동일\n",
    "  \n",
    "  # cost 계산\n",
    "  cost = F.mse_loss(prediction, y_train) # 평균 제곱 오차 함수(pyTorch내장)\n",
    "  # cost로 H(x) 개선하는 부분\n",
    "  optimizer.zero_grad() # gradient를 0으로 초기화\n",
    "  cost.backward() # 비용 함수를 미분하여 gradient 계산\n",
    "  optimizer.step() # W와 b를 업데이트\n",
    "  if epoch % 100 == 0:\n",
    "    # 100번마다 로그 출력\n",
    "    print('Epoch {:4d}/{} Cost: {:.6f}'.format(epoch, nb_epochs, cost.item()))\n",
    "# 임의의 입력 [73, 80, 75]를 선언\n",
    "new_var = torch.FloatTensor([[73, 80, 75]])\n",
    "# 입력한 값 [73, 80, 75]에 대해서 예측값 y를 리턴받아서 pred_y에 저장\n",
    "pred_y = model(new_var)\n",
    "print(\"훈련 후 입력이 73, 80, 75일 때의 예측값 :\", pred_y)\n",
    "print(list(model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "id": "c84182fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:    0 W: 0.881058 b: -0.264944 Cost: 19.612261\n",
      "Epoch:  400 W: 1.172251 b: 2.608432 Cost: 0.022209\n",
      "Epoch:  800 W: 1.025057 b: 2.943039 Cost: 0.000470\n",
      "Epoch: 1200 W: 1.003644 b: 2.991716 Cost: 0.000010\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "x_train = torch.FloatTensor([[1],[2],[3]])\n",
    "y_train = torch.FloatTensor([[4],[5],[6]])\n",
    "\n",
    "class LinearRegressionModel(nn.Module) :\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.linear = nn.Linear(1,1)\n",
    "    \n",
    "  def forward(self, x) :\n",
    "    return self.linear(x)\n",
    "  \n",
    "model = LinearRegressionModel()\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.02)\n",
    "\n",
    "nb_epochs = 1500\n",
    "\n",
    "for epoch in range(nb_epochs +1) :\n",
    "  # H(x) 계산\n",
    "  prediction = model(x_train)\n",
    "  # cost 계산\n",
    "  cost = F.mse_loss(prediction, y_train)\n",
    "  \n",
    "  optimizer.zero_grad()\n",
    "  cost.backward()\n",
    "  optimizer.step()\n",
    "  \n",
    "  if epoch % 400 == 0 :\n",
    "    W = model.linear.weight.item()\n",
    "    b = model.linear.bias.item()\n",
    "    \n",
    "    print(\"Epoch: {:4d} W: {:.6f} b: {:.6f} Cost: {:.6f}\".format(epoch, W, b, cost.item()))\n",
    "  \n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "id": "5cd68218",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>avgTemp</th>\n",
       "      <th>minTemp</th>\n",
       "      <th>maxTemp</th>\n",
       "      <th>rainFall</th>\n",
       "      <th>avgPrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20100101</td>\n",
       "      <td>-4.9</td>\n",
       "      <td>-11.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20100102</td>\n",
       "      <td>-3.1</td>\n",
       "      <td>-5.5</td>\n",
       "      <td>5.5</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20100103</td>\n",
       "      <td>-2.9</td>\n",
       "      <td>-6.9</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20100104</td>\n",
       "      <td>-1.8</td>\n",
       "      <td>-5.1</td>\n",
       "      <td>2.2</td>\n",
       "      <td>5.9</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20100105</td>\n",
       "      <td>-5.2</td>\n",
       "      <td>-8.7</td>\n",
       "      <td>-1.8</td>\n",
       "      <td>0.7</td>\n",
       "      <td>2060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>20100106</td>\n",
       "      <td>-7.3</td>\n",
       "      <td>-11.4</td>\n",
       "      <td>-2.5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>2060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>20100107</td>\n",
       "      <td>-6.7</td>\n",
       "      <td>-11.2</td>\n",
       "      <td>-1.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>20100108</td>\n",
       "      <td>-5.6</td>\n",
       "      <td>-11.4</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>20100109</td>\n",
       "      <td>-3.1</td>\n",
       "      <td>-8.8</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>20100110</td>\n",
       "      <td>-1.3</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>2.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2140</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       year  avgTemp  minTemp  maxTemp  rainFall  avgPrice\n",
       "0  20100101     -4.9    -11.0      0.9       0.0      2123\n",
       "1  20100102     -3.1     -5.5      5.5       0.8      2123\n",
       "2  20100103     -2.9     -6.9      1.4       0.0      2123\n",
       "3  20100104     -1.8     -5.1      2.2       5.9      2020\n",
       "4  20100105     -5.2     -8.7     -1.8       0.7      2060\n",
       "5  20100106     -7.3    -11.4     -2.5       0.3      2060\n",
       "6  20100107     -6.7    -11.2     -1.2       0.0      2140\n",
       "7  20100108     -5.6    -11.4      1.4       0.0      2140\n",
       "8  20100109     -3.1     -8.8      1.8       0.1      2140\n",
       "9  20100110     -1.3     -5.0      2.8       0.0      2140"
      ]
     },
     "execution_count": 379,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"data/price_data.csv\")\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608040a1",
   "metadata": {},
   "source": [
    "#### 실활용 예시 _ 기온, 강수량에 따른 채소값 예측모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "id": "665ded6e",
   "metadata": {
    "tags": [
     "lettuce_make_predict_model"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "X_train: torch.Size([2922, 4]) Y_train: torch.Size([2922, 1])\n",
      "Step:      0 | Loss: 15.406157 | pred[0]: -264.36\n",
      "Step:    500 | Loss: 11.770376 | pred[0]: -245.61\n",
      "Step:   1000 | Loss: 8.859745 | pred[0]: 251.73\n",
      "Step:   1500 | Loss: 6.539066 | pred[0]: 726.81\n",
      "Step:   2000 | Loss: 4.891608 | pred[0]: 1143.14\n",
      "Step:   2500 | Loss: 3.723203 | pred[0]: 1514.40\n",
      "Step:   3000 | Loss: 2.919070 | pred[0]: 1844.44\n",
      "Step:   3500 | Loss: 2.397035 | pred[0]: 2132.23\n",
      "Step:   4000 | Loss: 2.087150 | pred[0]: 2374.42\n",
      "Step:   4500 | Loss: 1.925621 | pred[0]: 2566.92\n",
      "Step:   5000 | Loss: 1.855600 | pred[0]: 2706.96\n",
      "Step:   5500 | Loss: 1.832079 | pred[0]: 2796.24\n",
      "Step:   6000 | Loss: 1.826438 | pred[0]: 2843.11\n",
      "Step:   6500 | Loss: 1.825520 | pred[0]: 2861.43\n",
      "Step:   7000 | Loss: 1.825392 | pred[0]: 2865.43\n",
      "Step:   7500 | Loss: 1.825359 | pred[0]: 2864.68\n",
      "Step:   8000 | Loss: 1.825348 | pred[0]: 2863.36\n",
      "Step:   8500 | Loss: 1.825345 | pred[0]: 2862.49\n",
      "Step:   9000 | Loss: 1.825344 | pred[0]: 2862.08\n",
      "Step:   9500 | Loss: 1.825344 | pred[0]: 2861.94\n",
      "Step:  10000 | Loss: 1.825344 | pred[0]: 2861.91\n",
      "\n",
      "학습된 모델을 저장했습니다. 파일명: data/saved_model.pt\n"
     ]
    }
   ],
   "source": [
    "# trainModel.py (핵심 부분만 교체/추가)\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# 1) 데이터 로드\n",
    "df = pd.read_csv(\"data/price_data.csv\")\n",
    "# 원하는 입력 칼럼을 명시적으로 선택하세요. (예: 평균/최저/최고/강수량)\n",
    "# df.columns 예시를 확인하고 정확히 맞추세요.\n",
    "# print(df.columns)\n",
    "\n",
    "# ★ 현재 3개 입력으로 학습 중이라면 예시:\n",
    "X_np = df[['avgTemp', 'minTemp', 'maxTemp','rainFall']].values.astype(np.float32)  # (N,3)\n",
    "y_np = df[['avgPrice']].values.astype(np.float32)                                       # (N,1)\n",
    "\n",
    "# 2) NaN/Inf 가드\n",
    "if np.isnan(X_np).any() or np.isinf(X_np).any():\n",
    "    raise ValueError(\"X에 NaN/Inf가 포함되어 있습니다.\")\n",
    "if np.isnan(y_np).any() or np.isinf(y_np).any():\n",
    "    raise ValueError(\"y에 NaN/Inf가 포함되어 있습니다.\")\n",
    "\n",
    "X = torch.from_numpy(X_np).to(device)\n",
    "y = torch.from_numpy(y_np).to(device)\n",
    "\n",
    "print(\"X_train:\", X.shape, \"Y_train:\", y.shape)  # (N,3) (N,1) 확인\n",
    "\n",
    "# 3) 정규화 (표준화)\n",
    "X_mean = X.mean(dim=0, keepdim=True)\n",
    "X_std  = X.std(dim=0, keepdim=True).clamp_min(1e-6)\n",
    "Xn = (X - X_mean) / X_std\n",
    "\n",
    "# (선택) 타깃 스케일 줄이기 → 안정적 학습에 도움\n",
    "y_scale = 1000.0\n",
    "yn = y / y_scale\n",
    "\n",
    "# 4) 모델\n",
    "class LinearRegressionModel(nn.Module):\n",
    "    def __init__(self, in_dim):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(in_dim, 1)\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "in_dim = Xn.shape[1]\n",
    "model = LinearRegressionModel(in_dim).to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# 5) 옵티마이저: Adam 권장 (SGD면 lr 더 낮추세요: 1e-5~1e-6)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "steps = 10000\n",
    "for step in range(steps + 1):\n",
    "    pred = model(Xn)               # 표준화된 입력 사용\n",
    "    loss = criterion(pred, yn)     # 표준화된 타깃 사용\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    # (선택) 그래디언트 클리핑으로 폭주 방지\n",
    "    nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    if step % 500 == 0:\n",
    "        # NaN 감시\n",
    "        if torch.isnan(loss):\n",
    "            raise RuntimeError(\"Loss가 NaN입니다. 입력/타깃/학습률을 점검하세요.\")\n",
    "        print(f\"Step: {step:6d} | Loss: {loss.item():.6f} | pred[0]: {(pred[0]*y_scale).item():.2f}\")\n",
    "\n",
    "# 6) 저장 전 폴더 보장\n",
    "if loss.item() < 2.0 :\n",
    "    os.makedirs(\"data\", exist_ok=True)\n",
    "    torch.save({\n",
    "        \"state_dict\": model.state_dict(),\n",
    "        \"X_mean\": X_mean.cpu(),\n",
    "        \"X_std\": X_std.cpu(),\n",
    "        \"y_scale\": y_scale\n",
    "    }, \"data/saved_model.pt\")\n",
    "    print(\"\\n학습된 모델을 저장했습니다. 파일명: data/saved_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "id": "2dc78c51",
   "metadata": {
    "tags": [
     "lettuec_predict_model_web"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "[경고] 체크포인트 입력 차원 불일치 (in_dim=4)\n",
      "✅ 모델 로드 완료 | 입력 차원: 4\n",
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on all addresses (0.0.0.0)\n",
      " * Running on http://127.0.0.1:8081\n",
      " * Running on http://192.168.0.99:8081\n",
      "Press CTRL+C to quit\n",
      "127.0.0.1 - - [14/Oct/2025 13:52:14] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [14/Oct/2025 13:52:14] \"GET /static/style.css HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [14/Oct/2025 13:52:28] \"POST / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [14/Oct/2025 13:52:29] \"GET /static/style.css HTTP/1.1\" 304 -\n",
      "127.0.0.1 - - [14/Oct/2025 13:52:46] \"POST / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [14/Oct/2025 13:52:46] \"GET /static/style.css HTTP/1.1\" 304 -\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, request, render_template\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import sys\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# ---- 모델 아키텍처 ----\n",
    "class LinearRegressionModel(nn.Module):\n",
    "    def __init__(self, in_dim: int):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(in_dim, 1)\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "# ---- 체크포인트 로드 ----\n",
    "try:\n",
    "    ckpt = torch.load('data/saved_model.pt', map_location=device)\n",
    "    state = ckpt[\"state_dict\"] if isinstance(ckpt, dict) and \"state_dict\" in ckpt else ckpt\n",
    "    in_dim = state[\"linear.weight\"].shape[1]\n",
    "\n",
    "    if in_dim != 5:\n",
    "        print(f\"[경고] 체크포인트 입력 차원 불일치 (in_dim={in_dim})\")\n",
    "    model = LinearRegressionModel(in_dim).to(device)\n",
    "    model.load_state_dict(state)\n",
    "    model.eval()\n",
    "\n",
    "    X_mean = torch.as_tensor(ckpt.get(\"X_mean\", 0), device=device) if isinstance(ckpt, dict) else None\n",
    "    X_std  = torch.as_tensor(ckpt.get(\"X_std\", 1), device=device) if isinstance(ckpt, dict) else None\n",
    "    y_scale = ckpt.get(\"y_scale\", 1.0) if isinstance(ckpt, dict) else 1.0\n",
    "\n",
    "    print(\"✅ 모델 로드 완료 | 입력 차원:\", in_dim)\n",
    "except FileNotFoundError:\n",
    "    print(\"❌ 오류: saved_model.pt를 찾을 수 없습니다.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# Flask 초기화\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route(\"/\", methods=['GET', 'POST'])\n",
    "def home():\n",
    "    predicted_price = None\n",
    "\n",
    "    if request.method == 'POST':\n",
    "        try:\n",
    "            avgTemp = float(request.form['avgTemp'])\n",
    "            minTemp = float(request.form['minTemp'])\n",
    "            maxTemp = float(request.form['maxTemp'])\n",
    "            rainfall = float(request.form['rainfall'])\n",
    "\n",
    "            x = torch.tensor([[avgTemp, minTemp, maxTemp, rainfall]], dtype=torch.float32, device=device)\n",
    "            if X_mean is not None and X_std is not None:\n",
    "                x = (x - X_mean) / X_std\n",
    "\n",
    "            with torch.no_grad():\n",
    "                y_pred = model(x) * y_scale\n",
    "                predicted_price = f\"{y_pred.item():,.2f}\"\n",
    "        except Exception as e:\n",
    "            return f\"에러 발생: {e} <a href='/'>돌아가기</a>\"\n",
    "\n",
    "    return render_template(\"index.html\", prediction=predicted_price)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(host='0.0.0.0', port=8081)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd687ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "x_data = [[1,2],[2,3],[3,1],[4,3],[5,3],[6,2]]\n",
    "y_data = [[0],[0],[0],[1],[1],[1]]\n",
    "x_train = torch.FloatTensor(x_data) # torch.Size([6, 2])\n",
    "y_train = torch.FloatTensor(y_data) # torch.Size([6, 1])\n",
    "\n",
    "class BinaryClassfier(nn.Module):\n",
    "  def __init__(self) :\n",
    "    super().__init__()\n",
    "    self.linear = nn.Linear(2,1)\n",
    "    self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "  def forward(self, x) :\n",
    "    return self.sigmoid(self.linear(x))\n",
    "  \n",
    "model = BinaryClassfier()\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=1)\n",
    "nb_epochs = 1000\n",
    "for epoch in range(nb_epochs + 1) :\n",
    "  hypothesis = model(x_train)\n",
    "  \n",
    "  cost = F.binary_cross_entropy(hypothesis, y_train)\n",
    "  \n",
    "  # cost 로 H(x) 개선\n",
    "  optimizer.zero_grad()\n",
    "  cost.backward()\n",
    "  optimizer.step()\n",
    "  \n",
    "  if epoch % 100 == 0 :\n",
    "    prediction = hypothesis >= torch.FloatTensor([0.5]) # 예측값이 0.5를 넘으면 True\n",
    "    correct_predication = prediction.float() == y_train\n",
    "    accuracy = correct_predication.sum().item() / len(correct_predication) #정확도\n",
    "    print(\"Epoch {:4d}/{}, Cost: {:.6f} Accuracy {:2.2f}%\".format(epoch,nb_epochs,cost.item(),accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2098c3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습된 모델에 임의의입력 [1,4] 를 적용하여 값을 검증\n",
    "new_var = torch.FloatTensor([1.0,4.0])\n",
    "pred_y = model(new_var) # forward 연산\n",
    "print(\"훈련 후 입력이 [1,4] 일때의 예측값 :\", pred_y)\n",
    "\n",
    "predition = pred_y >= torch.FloatTensor([0.5])\n",
    "print(predition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "id": "d53263d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이 이미지 데이터 레이블은 5이다.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_openml\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "mnist = fetch_openml('mnist_784', version=1, cache=True, as_frame=False)\n",
    "mnist.data[0]\n",
    "mnist.target[0]\n",
    "mnist.target = mnist.target.astype(np.int8)\n",
    "X = mnist.data / 255\n",
    "y = mnist.target\n",
    "\n",
    "plt.imshow(X[0].reshape(28,28), cmap='gray')\n",
    "print(\"이 이미지 데이터 레이블은 {:.0f}이다.\".format(y[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "id": "ca77ff46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:    1/3 Cost: 0.236985\n",
      "Epoch:    2/3 Cost: 0.305861\n",
      "Epoch:    3/3 Cost: 0.267922\n",
      "/n 테스트 데이터에서 예측 정확도 9614/10000 96\n",
      "예측결과 : 2\n",
      "이 이미지 데이터의 정답 레이블은 2\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1/7, random_state=0)\n",
    "\n",
    "# 텐서로 변환\n",
    "\n",
    "X_train = torch.Tensor(X_train)\n",
    "X_test = torch.Tensor(X_test)\n",
    "y_train = torch.Tensor(y_train)\n",
    "y_test = torch.Tensor(y_test)\n",
    "\n",
    "#TensorDataset 객체 생성\n",
    "\n",
    "ds_train = TensorDataset(X_train, y_train)\n",
    "ds_test = TensorDataset(X_test, y_test)\n",
    "\n",
    "# DataLoader 객체 생성\n",
    "\n",
    "loader_train = DataLoader(ds_train, batch_size=64, shuffle=True)\n",
    "loader_test = DataLoader(ds_test, batch_size=64, shuffle=True)\n",
    "\n",
    "\n",
    "model = nn.Sequential()\n",
    "model.add_module(\"fc1\", nn.Linear(28*28*1, 100))\n",
    "model.add_module(\"relu1\", nn.ReLU())\n",
    "model.add_module(\"fc2\", nn.Linear(100,100))\n",
    "model.add_module(\"relu2\", nn.ReLU())\n",
    "model.add_module(\"fc3\", nn.Linear(100,10))\n",
    "# print(model)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "epochs = 3\n",
    "\n",
    "for epoch in range(epochs) :\n",
    "  for data, targets in loader_train :\n",
    "    optimizer.zero_grad()\n",
    "    y_pred = model(data)\n",
    "    loss = loss_fn(y_pred, targets.long())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "  print(f\"Epoch: {epoch+1:4d}/{3} Cost: {loss.item():.6f}\")\n",
    "\n",
    "model.eval()\n",
    "correct = 0\n",
    "\n",
    "#데이터로더에서 미니배치를 하나씩 꺼내 추록을 수행\n",
    "\n",
    "with torch.no_grad() :\n",
    "  for data, targets in loader_test :\n",
    "    outputs = model(data)\n",
    "    # 추론계산\n",
    "    _, predicted = torch.max(outputs.data,1)\n",
    "    # 정답과 일치한 경우 정답 카운트를 증가\n",
    "    correct += (predicted == targets).sum().item()\n",
    "    \n",
    "# 정확도 출력\n",
    "\n",
    "data_num = len(loader_test.dataset)\n",
    "print(f\"/n 테스트 데이터에서 예측 정확도 {correct:.0f}/{data_num} {100*correct/data_num:.0f}\")\n",
    "\n",
    "index = 2018\n",
    "\n",
    "model.eval()\n",
    "data = X_test[index]\n",
    "ouput = model(data)\n",
    "\n",
    "_, predicted = torch.max(ouput.data, 0) # 확률이 가장 높은 레이블 계산\n",
    "print(f\"예측결과 : {predicted}\")\n",
    "\n",
    "X_test_show = (X_test[index]).numpy()\n",
    "plt.imshow(X_test_show.reshape(28,28), cmap='gray')\n",
    "print(f\"이 이미지 데이터의 정답 레이블은 {y_test[index]:.0f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "id": "0ff840c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0., 0., 2., 3.])\n"
     ]
    }
   ],
   "source": [
    "# relu test\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "relu = nn.ReLU()\n",
    "x = torch.tensor([-2.0, -1.0, 0.0, 2.0, 3.0])\n",
    "print(relu(x))\n",
    "# tensor([0., 0., 0., 2., 3.]) -> 음수 :0, 양수 : 통과"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edde0a78",
   "metadata": {},
   "source": [
    "#### RNN 구현해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "id": "6aa5abc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10, 8])\n",
      "torch.Size([1, 1, 8])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "input_size = 5\n",
    "hidden_size = 8\n",
    "\n",
    "# (batch_size, time_steps, input_size)\n",
    "\n",
    "inputs = torch.Tensor(1,10,input_size)\n",
    "cell = nn.RNN(input_size, hidden_size, batch_first=True)\n",
    "outputs, _status = cell(inputs)\n",
    "print(outputs.shape) # torch.Size([1, 10, 8])\n",
    "print(_status.shape) # torch.Size([1, 1, 8])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5979144",
   "metadata": {},
   "source": [
    "BRNN : 양방향순환신경망"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "id": "bd6b0cad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10, 8])\n",
      "torch.Size([2, 1, 8])\n"
     ]
    }
   ],
   "source": [
    "inputs = torch.Tensor(1,10,5)\n",
    "cell = nn.RNN(input_size=5, hidden_size =8, num_layers = 2, batch_first=True)\n",
    "outputs, _status = cell(inputs)\n",
    "print(outputs.shape)\n",
    "print(_status.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "id": "6ed0bd4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10, 16])\n",
      "torch.Size([4, 1, 8])\n"
     ]
    }
   ],
   "source": [
    "inputs = torch.Tensor(1,10,5)\n",
    "cell = nn.RNN(input_size= 5, hidden_size = 8, num_layers = 2, batch_first= True, bidirectional=True)\n",
    "outputs, _status = cell(inputs)\n",
    "print(outputs.shape)\n",
    "print(_status.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5cdb573",
   "metadata": {},
   "source": [
    "LSTM( Long-short Ter, Memory)\n",
    "\n",
    "- 바닐라 RNN 한계  -> 비교적 짧은 시퀀스에 대해서만 효과적\n",
    "- RNN의 time step이 길어질수록 앞의 정보가 뒤로 충분히 전달 못됨\n",
    "- 정보량이 소실되는 장기의존성 문제가 발생\n",
    "- 은닉층의 메모리셀에 입력게이트, 망각게이트 , 출력게이트를 추가해 불필요한 정보제거,유지할 정보 정함\n",
    "- 은닉층 계산식이 바닐라 RNN보다 복잡, cell-state 추가\n",
    "- 이를 통해 긴 시퀀스 처리에 탁월한 성능을 보임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "id": "2980c810",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.6.0'"
      ]
     },
     "execution_count": 423,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1bcb4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GRU(5, 8, batch_first=True)"
      ]
     },
     "execution_count": 429,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dim = 5\n",
    "hidden_size = 8\n",
    "\n",
    "nn.RNN(input_dim, hidden_size, batch_first=True)  # RNN(5, 8, batch_first=True)\n",
    "\n",
    "nn.LSTM(input_dim, hidden_size, batch_first=True) # LSTM(5, 8, batch_first=True)\n",
    "\n",
    "# GRU(Gated Recurrent Unit)\n",
    "\n",
    "nn.RNN(input_dim, hidden_size, batch_first=True)\n",
    "nn.GRU(input_dim, hidden_size, batch_first=True) # GRU(5, 8, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "id": "792cfbbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문자 집합의 크기 : 5\n",
      "{'!': 0, 'a': 1, 'e': 2, 'l': 3, 'p': 4}\n",
      "{0: '!', 1: 'a', 2: 'e', 3: 'l', 4: 'p'}\n",
      "[1, 4, 4, 3, 2]\n",
      "[4, 4, 3, 2, 0]\n",
      "[[1, 4, 4, 3, 2]]\n",
      "[[4, 4, 3, 2, 0]]\n",
      "[array([[0., 1., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 1.],\n",
      "       [0., 0., 0., 0., 1.],\n",
      "       [0., 0., 0., 1., 0.],\n",
      "       [0., 0., 1., 0., 0.]])]\n",
      "훈련 데이터의 크기 : torch.Size([1, 5, 5])\n",
      "레이블의 크기 : torch.Size([1, 5])\n",
      "torch.Size([1, 5, 5])\n",
      "torch.Size([5, 5])\n",
      "torch.Size([1, 5])\n",
      "torch.Size([5])\n",
      "99 loss:  0.0008274141582660377 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qb/k2smljtx419g49xbkmv9zs700000gn/T/ipykernel_30201/2010995795.py:41: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/miniforge3/conda-bld/libtorch_1741562938389/work/torch/csrc/utils/tensor_new.cpp:257.)\n",
      "  X = torch.FloatTensor(x_one_hot)\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "input_str = 'apple'\n",
    "label_str = 'pple!'\n",
    "char_vocab = sorted(list(set(input_str+label_str)))\n",
    "vocab_size = len(char_vocab)\n",
    "print ('문자 집합의 크기 : {}'.format(vocab_size))\n",
    "\n",
    "# 하이퍼파라메터 정의( 입력=원-핫 벡터이므로 입력의 크기=문자집합의 크기)\n",
    "input_size = vocab_size # 입력의 크기는 문자 집합의 크기\n",
    "hidden_size = 5\n",
    "output_size = 5\n",
    "learning_rate = 0.1\n",
    "char_to_index = dict((c, i) for i, c in enumerate(char_vocab)) # 문자에 고유한 정수 인덱스 부여\n",
    "print(char_to_index)\n",
    "\n",
    "index_to_char={}\n",
    "for key, value in char_to_index.items():\n",
    "  index_to_char[value] = key\n",
    "print(index_to_char)\n",
    "\n",
    "x_data = [char_to_index[c] for c in input_str]\n",
    "y_data = [char_to_index[c] for c in label_str]\n",
    "print(x_data)\n",
    "print(y_data)\n",
    "# pytorch의 nn.RNN()은 기본적으로 3차원 텐서를 입력값으로 받는다.\n",
    "#  배치 차원 추가필요\n",
    "# 텐서 연산인 unsqueeze(0)를 통해 해결할 수도 있었음.\n",
    "x_data = [x_data]\n",
    "y_data = [y_data]\n",
    "print(x_data)\n",
    "print(y_data)\n",
    "# 입력 sequence의 각 문자들을 원-핫 벡터로 변경\n",
    "x_one_hot = [np.eye(vocab_size)[x] for x in x_data]\n",
    "print(x_one_hot)\n",
    "# 이후, 입력데이터와 레이블데이터를 tensor로 변환\n",
    "X = torch.FloatTensor(x_one_hot)\n",
    "Y = torch.LongTensor(y_data)\n",
    "print('훈련 데이터의 크기 : {}'.format(X.shape))\n",
    "print('레이블의 크기 : {}'.format(Y.shape))\n",
    "\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "  def __init__(self, input_size, hidden_size, output_size):\n",
    "    super(Net, self).__init__()\n",
    "    self.rnn = torch.nn.RNN(input_size, hidden_size, batch_first=True) # RNN셀 구현\n",
    "    self.fc = torch.nn.Linear(hidden_size, output_size, bias=True) # 출력층 구현\n",
    "  def forward(self, x): # 구현한 RNN 셀과 출력층을 연결\n",
    "    x, _status = self.rnn(x)\n",
    "    x = self.fc(x)\n",
    "    return x\n",
    "\n",
    "net = Net(input_size, hidden_size, output_size)\n",
    "outputs = net(X)\n",
    "print(outputs.shape) # 3차원 텐서\n",
    "print(outputs.view(-1, input_size).shape) # 2차원 텐서로 변환\n",
    "print(Y.shape)\n",
    "print(Y.view(-1).shape)\n",
    "\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), learning_rate)\n",
    "\n",
    "\n",
    "for i in range(100):\n",
    "  optimizer.zero_grad()\n",
    "  outputs = net(X)\n",
    "  loss = criterion(outputs.view(-1, input_size), Y.view(-1)) # view를 하는 이유는 Batch 차원 제거를 위해\n",
    "  loss.backward() # 기울기 계산\n",
    "  optimizer.step() # 아까 optimizer 선언 시 넣어둔 파라미터 업데이트\n",
    "  # 아래 세 줄은 모델이 실제 어떻게 예측했는지를 확인하기 위한 코드.\n",
    "\n",
    "result = outputs.data.numpy().argmax(axis=2) # 최종 예측값인 각 time-step 별 5차원 벡터에 대해서 가장 높은 값의 인덱스를 선택\n",
    "\n",
    "result_str = ''.join([index_to_char[c] for c in np.squeeze(result)])\n",
    "print(i, \"loss: \", loss.item(), \"prediction: \", result, \"true Y: \", y_data,\"prediction str: \", result_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "id": "9aaf73af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'.': 0, 'p': 1, 'u': 2, 'f': 3, 'w': 4, 'b': 5, ',': 6, 'a': 7, 'm': 8, 'k': 9, 'c': 10, 'l': 11, 'g': 12, 's': 13, 'n': 14, 't': 15, ' ': 16, \"'\": 17, 'o': 18, 'y': 19, 'r': 20, 'e': 21, 'h': 22, 'd': 23, 'i': 24}\n",
      "문자 집합의 크기 : 25\n",
      "0 if you wan -> f you want\n",
      "1 f you want ->  you want \n",
      "2  you want  -> you want t\n",
      "3 you want t -> ou want to\n",
      "4 ou want to -> u want to \n",
      "5 u want to  ->  want to b\n",
      "6  want to b -> want to bu\n",
      "7 want to bu -> ant to bui\n",
      "8 ant to bui -> nt to buil\n",
      "9 nt to buil -> t to build\n",
      "10 t to build ->  to build \n",
      "11  to build  -> to build a\n",
      "12 to build a -> o build a \n",
      "13 o build a  ->  build a s\n",
      "14  build a s -> build a sh\n",
      "15 build a sh -> uild a shi\n",
      "16 uild a shi -> ild a ship\n",
      "17 ild a ship -> ld a ship,\n",
      "18 ld a ship, -> d a ship, \n",
      "19 d a ship,  ->  a ship, d\n",
      "20  a ship, d -> a ship, do\n",
      "21 a ship, do ->  ship, don\n",
      "22  ship, don -> ship, don'\n",
      "23 ship, don' -> hip, don't\n",
      "24 hip, don't -> ip, don't \n",
      "25 ip, don't  -> p, don't d\n",
      "26 p, don't d -> , don't dr\n",
      "27 , don't dr ->  don't dru\n",
      "28  don't dru -> don't drum\n",
      "29 don't drum -> on't drum \n",
      "30 on't drum  -> n't drum u\n",
      "31 n't drum u -> 't drum up\n",
      "32 't drum up -> t drum up \n",
      "33 t drum up  ->  drum up p\n",
      "34  drum up p -> drum up pe\n",
      "35 drum up pe -> rum up peo\n",
      "36 rum up peo -> um up peop\n",
      "37 um up peop -> m up peopl\n",
      "38 m up peopl ->  up people\n",
      "39  up people -> up people \n",
      "40 up people  -> p people t\n",
      "41 p people t ->  people to\n",
      "42  people to -> people tog\n",
      "43 people tog -> eople toge\n",
      "44 eople toge -> ople toget\n",
      "45 ople toget -> ple togeth\n",
      "46 ple togeth -> le togethe\n",
      "47 le togethe -> e together\n",
      "48 e together ->  together \n",
      "49  together  -> together t\n",
      "50 together t -> ogether to\n",
      "51 ogether to -> gether to \n",
      "52 gether to  -> ether to c\n",
      "53 ether to c -> ther to co\n",
      "54 ther to co -> her to col\n",
      "55 her to col -> er to coll\n",
      "56 er to coll -> r to colle\n",
      "57 r to colle ->  to collec\n",
      "58  to collec -> to collect\n",
      "59 to collect -> o collect \n",
      "60 o collect  ->  collect w\n",
      "61  collect w -> collect wo\n",
      "62 collect wo -> ollect woo\n",
      "63 ollect woo -> llect wood\n",
      "64 llect wood -> lect wood \n",
      "65 lect wood  -> ect wood a\n",
      "66 ect wood a -> ct wood an\n",
      "67 ct wood an -> t wood and\n",
      "68 t wood and ->  wood and \n",
      "69  wood and  -> wood and d\n",
      "70 wood and d -> ood and do\n",
      "71 ood and do -> od and don\n",
      "72 od and don -> d and don'\n",
      "73 d and don' ->  and don't\n",
      "74  and don't -> and don't \n",
      "75 and don't  -> nd don't a\n",
      "76 nd don't a -> d don't as\n",
      "77 d don't as ->  don't ass\n",
      "78  don't ass -> don't assi\n",
      "79 don't assi -> on't assig\n",
      "80 on't assig -> n't assign\n",
      "81 n't assign -> 't assign \n",
      "82 't assign  -> t assign t\n",
      "83 t assign t ->  assign th\n",
      "84  assign th -> assign the\n",
      "85 assign the -> ssign them\n",
      "86 ssign them -> sign them \n",
      "87 sign them  -> ign them t\n",
      "88 ign them t -> gn them ta\n",
      "89 gn them ta -> n them tas\n",
      "90 n them tas ->  them task\n",
      "91  them task -> them tasks\n",
      "92 them tasks -> hem tasks \n",
      "93 hem tasks  -> em tasks a\n",
      "94 em tasks a -> m tasks an\n",
      "95 m tasks an ->  tasks and\n",
      "96  tasks and -> tasks and \n",
      "97 tasks and  -> asks and w\n",
      "98 asks and w -> sks and wo\n",
      "99 sks and wo -> ks and wor\n",
      "100 ks and wor -> s and work\n",
      "101 s and work ->  and work,\n",
      "102  and work, -> and work, \n",
      "103 and work,  -> nd work, b\n",
      "104 nd work, b -> d work, bu\n",
      "105 d work, bu ->  work, but\n",
      "106  work, but -> work, but \n",
      "107 work, but  -> ork, but r\n",
      "108 ork, but r -> rk, but ra\n",
      "109 rk, but ra -> k, but rat\n",
      "110 k, but rat -> , but rath\n",
      "111 , but rath ->  but rathe\n",
      "112  but rathe -> but rather\n",
      "113 but rather -> ut rather \n",
      "114 ut rather  -> t rather t\n",
      "115 t rather t ->  rather te\n",
      "116  rather te -> rather tea\n",
      "117 rather tea -> ather teac\n",
      "118 ather teac -> ther teach\n",
      "119 ther teach -> her teach \n",
      "120 her teach  -> er teach t\n",
      "121 er teach t -> r teach th\n",
      "122 r teach th ->  teach the\n",
      "123  teach the -> teach them\n",
      "124 teach them -> each them \n",
      "125 each them  -> ach them t\n",
      "126 ach them t -> ch them to\n",
      "127 ch them to -> h them to \n",
      "128 h them to  ->  them to l\n",
      "129  them to l -> them to lo\n",
      "130 them to lo -> hem to lon\n",
      "131 hem to lon -> em to long\n",
      "132 em to long -> m to long \n",
      "133 m to long  ->  to long f\n",
      "134  to long f -> to long fo\n",
      "135 to long fo -> o long for\n",
      "136 o long for ->  long for \n",
      "137  long for  -> long for t\n",
      "138 long for t -> ong for th\n",
      "139 ong for th -> ng for the\n",
      "140 ng for the -> g for the \n",
      "141 g for the  ->  for the e\n",
      "142  for the e -> for the en\n",
      "143 for the en -> or the end\n",
      "144 or the end -> r the endl\n",
      "145 r the endl ->  the endle\n",
      "146  the endle -> the endles\n",
      "147 the endles -> he endless\n",
      "148 he endless -> e endless \n",
      "149 e endless  ->  endless i\n",
      "150  endless i -> endless im\n",
      "151 endless im -> ndless imm\n",
      "152 ndless imm -> dless imme\n",
      "153 dless imme -> less immen\n",
      "154 less immen -> ess immens\n",
      "155 ess immens -> ss immensi\n",
      "156 ss immensi -> s immensit\n",
      "157 s immensit ->  immensity\n",
      "158  immensity -> immensity \n",
      "159 immensity  -> mmensity o\n",
      "160 mmensity o -> mensity of\n",
      "161 mensity of -> ensity of \n",
      "162 ensity of  -> nsity of t\n",
      "163 nsity of t -> sity of th\n",
      "164 sity of th -> ity of the\n",
      "165 ity of the -> ty of the \n",
      "166 ty of the  -> y of the s\n",
      "167 y of the s ->  of the se\n",
      "168  of the se -> of the sea\n",
      "169 of the sea -> f the sea.\n",
      "[24, 3, 16, 19, 18, 2, 16, 4, 7, 14]\n",
      "[3, 16, 19, 18, 2, 16, 4, 7, 14, 15]\n",
      "훈련 데이터의 크기 : torch.Size([170, 10, 25])\n",
      "레이블의 크기 : torch.Size([170, 10])\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0.]])\n",
      "tensor([ 3, 16, 19, 18,  2, 16,  4,  7, 14, 15])\n",
      "torch.Size([170, 10, 25])\n",
      "torch.Size([1700, 25])\n",
      "torch.Size([170, 10])\n",
      "torch.Size([1700])\n",
      "t you want \n",
      "t you want t\n",
      "t you want to\n",
      "t you want to \n",
      "t you want to b\n",
      "t you want to bu\n",
      "t you want to bui\n",
      "t you want to buil\n",
      "t you want to build\n",
      "t you want to build \n",
      "t you want to build a\n",
      "t you want to build a \n",
      "t you want to build a s\n",
      "t you want to build a sh\n",
      "t you want to build a shi\n",
      "t you want to build a ship\n",
      "t you want to build a ship,\n",
      "t you want to build a ship, \n",
      "t you want to build a ship, d\n",
      "t you want to build a ship, do\n",
      "t you want to build a ship, don\n",
      "t you want to build a ship, don'\n",
      "t you want to build a ship, don't\n",
      "t you want to build a ship, don't \n",
      "t you want to build a ship, don't d\n",
      "t you want to build a ship, don't dr\n",
      "t you want to build a ship, don't dru\n",
      "t you want to build a ship, don't drum\n",
      "t you want to build a ship, don't drum \n",
      "t you want to build a ship, don't drum u\n",
      "t you want to build a ship, don't drum up\n",
      "t you want to build a ship, don't drum up \n",
      "t you want to build a ship, don't drum up p\n",
      "t you want to build a ship, don't drum up pe\n",
      "t you want to build a ship, don't drum up peo\n",
      "t you want to build a ship, don't drum up peop\n",
      "t you want to build a ship, don't drum up peopl\n",
      "t you want to build a ship, don't drum up people\n",
      "t you want to build a ship, don't drum up people \n",
      "t you want to build a ship, don't drum up people t\n",
      "t you want to build a ship, don't drum up people to\n",
      "t you want to build a ship, don't drum up people tog\n",
      "t you want to build a ship, don't drum up people toge\n",
      "t you want to build a ship, don't drum up people toget\n",
      "t you want to build a ship, don't drum up people togeth\n",
      "t you want to build a ship, don't drum up people togethe\n",
      "t you want to build a ship, don't drum up people together\n",
      "t you want to build a ship, don't drum up people together \n",
      "t you want to build a ship, don't drum up people together t\n",
      "t you want to build a ship, don't drum up people together te\n",
      "t you want to build a ship, don't drum up people together te \n",
      "t you want to build a ship, don't drum up people together te c\n",
      "t you want to build a ship, don't drum up people together te co\n",
      "t you want to build a ship, don't drum up people together te col\n",
      "t you want to build a ship, don't drum up people together te coll\n",
      "t you want to build a ship, don't drum up people together te colle\n",
      "t you want to build a ship, don't drum up people together te collec\n",
      "t you want to build a ship, don't drum up people together te collect\n",
      "t you want to build a ship, don't drum up people together te collect \n",
      "t you want to build a ship, don't drum up people together te collect w\n",
      "t you want to build a ship, don't drum up people together te collect wo\n",
      "t you want to build a ship, don't drum up people together te collect woo\n",
      "t you want to build a ship, don't drum up people together te collect wood\n",
      "t you want to build a ship, don't drum up people together te collect wood \n",
      "t you want to build a ship, don't drum up people together te collect wood a\n",
      "t you want to build a ship, don't drum up people together te collect wood an\n",
      "t you want to build a ship, don't drum up people together te collect wood and\n",
      "t you want to build a ship, don't drum up people together te collect wood and \n",
      "t you want to build a ship, don't drum up people together te collect wood and d\n",
      "t you want to build a ship, don't drum up people together te collect wood and do\n",
      "t you want to build a ship, don't drum up people together te collect wood and don\n",
      "t you want to build a ship, don't drum up people together te collect wood and don'\n",
      "t you want to build a ship, don't drum up people together te collect wood and don't\n",
      "t you want to build a ship, don't drum up people together te collect wood and don't \n",
      "t you want to build a ship, don't drum up people together te collect wood and don't d\n",
      "t you want to build a ship, don't drum up people together te collect wood and don't ds\n",
      "t you want to build a ship, don't drum up people together te collect wood and don't dss\n",
      "t you want to build a ship, don't drum up people together te collect wood and don't dssi\n",
      "t you want to build a ship, don't drum up people together te collect wood and don't dssig\n",
      "t you want to build a ship, don't drum up people together te collect wood and don't dssign\n",
      "t you want to build a ship, don't drum up people together te collect wood and don't dssign \n",
      "t you want to build a ship, don't drum up people together te collect wood and don't dssign t\n",
      "t you want to build a ship, don't drum up people together te collect wood and don't dssign th\n",
      "t you want to build a ship, don't drum up people together te collect wood and don't dssign the\n",
      "t you want to build a ship, don't drum up people together te collect wood and don't dssign them\n",
      "t you want to build a ship, don't drum up people together te collect wood and don't dssign them \n",
      "t you want to build a ship, don't drum up people together te collect wood and don't dssign them t\n",
      "t you want to build a ship, don't drum up people together te collect wood and don't dssign them to\n",
      "t you want to build a ship, don't drum up people together te collect wood and don't dssign them tos\n",
      "t you want to build a ship, don't drum up people together te collect wood and don't dssign them tosk\n",
      "t you want to build a ship, don't drum up people together te collect wood and don't dssign them tosks\n",
      "t you want to build a ship, don't drum up people together te collect wood and don't dssign them tosks \n",
      "t you want to build a ship, don't drum up people together te collect wood and don't dssign them tosks a\n",
      "t you want to build a ship, don't drum up people together te collect wood and don't dssign them tosks an\n",
      "t you want to build a ship, don't drum up people together te collect wood and don't dssign them tosks and\n",
      "t you want to build a ship, don't drum up people together te collect wood and don't dssign them tosks and \n",
      "t you want to build a ship, don't drum up people together te collect wood and don't dssign them tosks and w\n",
      "t you want to build a ship, don't drum up people together te collect wood and don't dssign them tosks and wo\n",
      "t you want to build a ship, don't drum up people together te collect wood and don't dssign them tosks and wor\n",
      "t you want to build a ship, don't drum up people together te collect wood and don't dssign them tosks and work\n",
      "t you want to build a ship, don't drum up people together te collect wood and don't dssign them tosks and work,\n",
      "t you want to build a ship, don't drum up people together te collect wood and don't dssign them tosks and work, \n",
      "t you want to build a ship, don't drum up people together te collect wood and don't dssign them tosks and work, b\n",
      "t you want to build a ship, don't drum up people together te collect wood and don't dssign them tosks and work, bu\n",
      "t you want to build a ship, don't drum up people together te collect wood and don't dssign them tosks and work, but\n",
      "t you want to build a ship, don't drum up people together te collect wood and don't dssign them tosks and work, but \n",
      "t you want to build a ship, don't drum up people together te collect wood and don't dssign them tosks and work, but r\n",
      "t you want to build a ship, don't drum up people together te collect wood and don't dssign them tosks and work, but ra\n",
      "t you want to build a ship, don't drum up people together te collect wood and don't dssign them tosks and work, but rat\n",
      "t you want to build a ship, don't drum up people together te collect wood and don't dssign them tosks and work, but rath\n",
      "t you want to build a ship, don't drum up people together te collect wood and don't dssign them tosks and work, but rathe\n",
      "t you want to build a ship, don't drum up people together te collect wood and don't dssign them tosks and work, but rather\n",
      "t you want to build a ship, don't drum up people together te collect wood and don't dssign them tosks and work, but rather \n",
      "t you want to build a ship, don't drum up people together te collect wood and don't dssign them tosks and work, but rather t\n",
      "t you want to build a ship, don't drum up people together te collect wood and don't dssign them tosks and work, but rather te\n",
      "t you want to build a ship, don't drum up people together te collect wood and don't dssign them tosks and work, but rather tea\n",
      "t you want to build a ship, don't drum up people together te collect wood and don't dssign them tosks and work, but rather teac\n",
      "t you want to build a ship, don't drum up people together te collect wood and don't dssign them tosks and work, but rather teach\n",
      "t you want to build a ship, don't drum up people together te collect wood and don't dssign them tosks and work, but rather teach \n",
      "t you want to build a ship, don't drum up people together te collect wood and don't dssign them tosks and work, but rather teach t\n",
      "t you want to build a ship, don't drum up people together te collect wood and don't dssign them tosks and work, but rather teach th\n",
      "t you want to build a ship, don't drum up people together te collect wood and don't dssign them tosks and work, but rather teach the\n",
      "t you want to build a ship, don't drum up people together te collect wood and don't dssign them tosks and work, but rather teach them\n",
      "t you want to build a ship, don't drum up people together te collect wood and don't dssign them tosks and work, but rather teach them \n",
      "t you want to build a ship, don't drum up people together te collect wood and don't dssign them tosks and work, but rather teach them t\n",
      "t you want to build a ship, don't drum up people together te collect wood and don't dssign them tosks and work, but rather teach them to\n",
      "t you want to build a ship, don't drum up people together te collect wood and don't dssign them tosks and work, but rather teach them to \n",
      "t you want to build a ship, don't drum up people together te collect wood and don't dssign them tosks and work, but rather teach them to l\n",
      "t you want to build a ship, don't drum up people together te collect wood and don't dssign them tosks and work, but rather teach them to lo\n",
      "t you want to build a ship, don't drum up people together te collect wood and don't dssign them tosks and work, but rather teach them to lon\n",
      "t you want to build a ship, don't drum up people together te collect wood and don't dssign them tosks and work, but rather teach them to long\n",
      "t you want to build a ship, don't drum up people together te collect wood and don't dssign them tosks and work, but rather teach them to long \n",
      "t you want to build a ship, don't drum up people together te collect wood and don't dssign them tosks and work, but rather teach them to long f\n",
      "t you want to build a ship, don't drum up people together te collect wood and don't dssign them tosks and work, but rather teach them to long fo\n",
      "t you want to build a ship, don't drum up people together te collect wood and don't dssign them tosks and work, but rather teach them to long for\n",
      "t you want to build a ship, don't drum up people together te collect wood and don't dssign them tosks and work, but rather teach them to long for \n",
      "t you want to build a ship, don't drum up people together te collect wood and don't dssign them tosks and work, but rather teach them to long for t\n",
      "t you want to build a ship, don't drum up people together te collect wood and don't dssign them tosks and work, but rather teach them to long for th\n",
      "t you want to build a ship, don't drum up people together te collect wood and don't dssign them tosks and work, but rather teach them to long for the\n",
      "t you want to build a ship, don't drum up people together te collect wood and don't dssign them tosks and work, but rather teach them to long for the \n",
      "t you want to build a ship, don't drum up people together te collect wood and don't dssign them tosks and work, but rather teach them to long for the e\n",
      "t you want to build a ship, don't drum up people together te collect wood and don't dssign them tosks and work, but rather teach them to long for the en\n",
      "t you want to build a ship, don't drum up people together te collect wood and don't dssign them tosks and work, but rather teach them to long for the end\n",
      "t you want to build a ship, don't drum up people together te collect wood and don't dssign them tosks and work, but rather teach them to long for the endl\n",
      "t you want to build a ship, don't drum up people together te collect wood and don't dssign them tosks and work, but rather teach them to long for the endle\n",
      "t you want to build a ship, don't drum up people together te collect wood and don't dssign them tosks and work, but rather teach them to long for the endles\n",
      "t you want to build a ship, don't drum up people together te collect wood and don't dssign them tosks and work, but rather teach them to long for the endless\n",
      "t you want to build a ship, don't drum up people together te collect wood and don't dssign them tosks and work, but rather teach them to long for the endless \n",
      "t you want to build a ship, don't drum up people together te collect wood and don't dssign them tosks and work, but rather teach them to long for the endless i\n",
      "t you want to build a ship, don't drum up people together te collect wood and don't dssign them tosks and work, but rather teach them to long for the endless im\n",
      "t you want to build a ship, don't drum up people together te collect wood and don't dssign them tosks and work, but rather teach them to long for the endless imm\n",
      "t you want to build a ship, don't drum up people together te collect wood and don't dssign them tosks and work, but rather teach them to long for the endless imme\n",
      "t you want to build a ship, don't drum up people together te collect wood and don't dssign them tosks and work, but rather teach them to long for the endless immen\n",
      "t you want to build a ship, don't drum up people together te collect wood and don't dssign them tosks and work, but rather teach them to long for the endless immens\n",
      "t you want to build a ship, don't drum up people together te collect wood and don't dssign them tosks and work, but rather teach them to long for the endless immensi\n",
      "t you want to build a ship, don't drum up people together te collect wood and don't dssign them tosks and work, but rather teach them to long for the endless immensit\n",
      "t you want to build a ship, don't drum up people together te collect wood and don't dssign them tosks and work, but rather teach them to long for the endless immensity\n",
      "t you want to build a ship, don't drum up people together te collect wood and don't dssign them tosks and work, but rather teach them to long for the endless immensity \n",
      "t you want to build a ship, don't drum up people together te collect wood and don't dssign them tosks and work, but rather teach them to long for the endless immensity o\n",
      "t you want to build a ship, don't drum up people together te collect wood and don't dssign them tosks and work, but rather teach them to long for the endless immensity of\n",
      "t you want to build a ship, don't drum up people together te collect wood and don't dssign them tosks and work, but rather teach them to long for the endless immensity of \n",
      "t you want to build a ship, don't drum up people together te collect wood and don't dssign them tosks and work, but rather teach them to long for the endless immensity of t\n",
      "t you want to build a ship, don't drum up people together te collect wood and don't dssign them tosks and work, but rather teach them to long for the endless immensity of th\n",
      "t you want to build a ship, don't drum up people together te collect wood and don't dssign them tosks and work, but rather teach them to long for the endless immensity of the\n",
      "t you want to build a ship, don't drum up people together te collect wood and don't dssign them tosks and work, but rather teach them to long for the endless immensity of the \n",
      "t you want to build a ship, don't drum up people together te collect wood and don't dssign them tosks and work, but rather teach them to long for the endless immensity of the s\n",
      "t you want to build a ship, don't drum up people together te collect wood and don't dssign them tosks and work, but rather teach them to long for the endless immensity of the se\n",
      "t you want to build a ship, don't drum up people together te collect wood and don't dssign them tosks and work, but rather teach them to long for the endless immensity of the sea\n",
      "t you want to build a ship, don't drum up people together te collect wood and don't dssign them tosks and work, but rather teach them to long for the endless immensity of the sea.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "# 훈련데터의 처리\n",
    "sentence = (\"if you want to build a ship, don't drum up people together to \"\n",
    "\"collect wood and don't assign them tasks and work, but rather \"\n",
    "\"teach them to long for the endless immensity of the sea.\")\n",
    "\n",
    "char_set = list(set(sentence)) # 중복을 제거한 문자 집합 생성\n",
    "char_dic = {c: i for i, c in enumerate(char_set)} # 각 문자에 정수 인코딩\n",
    "print(char_dic) # 공백도 여기서는 하나의 원소\n",
    "\n",
    "dic_size = len(char_dic)\n",
    "print('문자 집합의 크기 : {}'.format(dic_size))\n",
    "\n",
    "# 하이퍼파라미터 설정, hidden_size(은닉상태의 크기)를 입력의 크기와 동일(다른값도 무방함)\n",
    "hidden_size = dic_size\n",
    "sequence_length = 10 # 임의 숫자 지정\n",
    "learning_rate = 0.1\n",
    "# 데이터 구성\n",
    "x_data = []\n",
    "y_data = []\n",
    "\n",
    "# sequence_length값인 10의 단위로 샘플을 잘라 데이터를 보여줌\n",
    "for i in range(0, len(sentence) - sequence_length):\n",
    "  x_str = sentence[i:i + sequence_length]\n",
    "  y_str = sentence[i + 1: i + sequence_length + 1]\n",
    "  print(i, x_str, '->', y_str)\n",
    "  x_data.append([char_dic[c] for c in x_str]) # x str to index\n",
    "  y_data.append([char_dic[c] for c in y_str]) # y str to index\n",
    "# 이상, 170개의 샘플이 생성되었으며, 각 샘플의 각 문자들은 고유한 정수로인코딩이 된 상태. 이중 0번째 레이블데이터와 데이터를 출력\n",
    "print(x_data[0])\n",
    "print(y_data[0])\n",
    "# 입력 시퀀스에 대하여 원-핫 인코딩을 수행후, 입력데이터와 레이블데이터를 텐서로 변환\n",
    "x_one_hot = [np.eye(dic_size)[x] for x in x_data] # x 데이터는 원-핫 인코딩\n",
    "X = torch.FloatTensor(x_one_hot)\n",
    "Y = torch.LongTensor(y_data)\n",
    "print('훈련 데이터의 크기 : {}'.format(X.shape))\n",
    "print('레이블의 크기 : {}'.format(Y.shape))\n",
    "# 원-핫 인코딩 된 결과중 첫번째 샘플만 출력\n",
    "print(X[0])\n",
    "print(Y[0])\n",
    "\n",
    "# 모델 설계부분, 은닉층은 2개,\n",
    "class Net(torch.nn.Module):\n",
    "  def __init__(self, input_dim, hidden_dim, layers): # 현재 hidden_size는dic_size와 같음.\n",
    "    super(Net, self).__init__()\n",
    "    self.rnn = torch.nn.RNN(input_dim, hidden_dim, num_layers=layers,batch_first=True)\n",
    "    self.fc = torch.nn.Linear(hidden_dim, hidden_dim, bias=True)\n",
    "  def forward(self, x):\n",
    "    x, _status = self.rnn(x)\n",
    "    x = self.fc(x)\n",
    "    return x\n",
    "\n",
    "net = Net(dic_size, hidden_size, 2) # 은닉층을 두 개 쌓습니다.\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), learning_rate)\n",
    "\n",
    "# 모델에 입력을 넣은 후 출력의 크기를 확인\n",
    "outputs = net(X)\n",
    "print(outputs.shape) # 3차원 텐서\n",
    "print(outputs.view(-1, dic_size).shape) # 2차원 텐서로 변환.\n",
    "print(Y.shape)\n",
    "print(Y.view(-1).shape)\n",
    "\n",
    "for i in range(100):\n",
    "  optimizer.zero_grad()\n",
    "  outputs = net(X) # (170, 10, 25) 크기를 가진 텐서를 매 에포크마다 모델의 입력으로 사용\n",
    "  loss = criterion(outputs.view(-1, dic_size), Y.view(-1))\n",
    "  loss.backward()\n",
    "  optimizer.step()\n",
    "# results의 텐서 크기는 (170, 10)\n",
    "  results = outputs.argmax(dim=2)\n",
    "\n",
    "predict_str = \"\"\n",
    "for j, result in enumerate(results):\n",
    "  if j == 0: # 처음에는 예측 결과를 전부 가져오지만\n",
    "    predict_str += ''.join([char_set[t] for t in result])\n",
    "  else: # 그 다음에는 마지막 글자만 반복 추가\n",
    "    predict_str += char_set[result[-1]]\n",
    "    print(predict_str)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langCh-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
